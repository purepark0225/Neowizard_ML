{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d790f831",
   "metadata": {},
   "source": [
    "### XOR 문제 - 딥러닝 아키텍처\n",
    "- 딥러닝에서는, 1개 이상의 은닉층(hidden layer)을 만들 수 있고, 각 은닉층(hidden layer)에 존재하는 노드(node) 개수 또한 임의의 개수를 만들 수 있음. 그러나 은닉층과 노드 수가 많아지면 학습 속도가 느려지므로 적절한 개수의 은닉층과 노드수를 고려하여 구현하는 것이 필요함\n",
    "\n",
    "1) input\n",
    "\n",
    "2) feed forward\n",
    "\n",
    "3) 정답 t와 출력된 y 값의 차이를 바탕으로 손실(loss) 값 계산\n",
    "\n",
    "4) update W(2), b(2), W(3), b(3) => repeat 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362cdc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND object is created\n",
      "Initial loss value =  8.921548369928487\n",
      "step =  0  , loss value =  3.1289477543902056\n",
      "step =  400  , loss value =  0.025480318493430833\n",
      "step =  800  , loss value =  0.016779688015569616\n",
      "step =  1200  , loss value =  0.014842850775500026\n",
      "step =  1600  , loss value =  0.01409844762239433\n",
      "step =  2000  , loss value =  0.013689463445428046\n",
      "step =  2400  , loss value =  0.013428888478155895\n",
      "step =  2800  , loss value =  0.013247907019988075\n",
      "step =  3200  , loss value =  0.013114727311191648\n",
      "step =  3600  , loss value =  0.01301254351387229\n",
      "step =  4000  , loss value =  0.012931618237166605\n",
      "step =  4400  , loss value =  0.012865914854010672\n",
      "step =  4800  , loss value =  0.012811489797600854\n",
      "step =  5200  , loss value =  0.012765656613187987\n",
      "step =  5600  , loss value =  0.012726521320140216\n",
      "step =  6000  , loss value =  0.012692709840699033\n",
      "step =  6400  , loss value =  0.012663200792994203\n",
      "step =  6800  , loss value =  0.012637218979921254\n",
      "step =  7200  , loss value =  0.012614165316208358\n",
      "step =  7600  , loss value =  0.012593569427866914\n",
      "step =  8000  , loss value =  0.01257505679477008\n",
      "step =  8400  , loss value =  0.012558325479759965\n",
      "step =  8800  , loss value =  0.012543129331875482\n",
      "step =  9200  , loss value =  0.012529265652164206\n",
      "step =  9600  , loss value =  0.012516565996387974\n",
      "step =  10000  , loss value =  0.01250488922247555\n",
      "(array([1.31145734e-47]), 0)\n",
      "(array([0.00061006]), 0)\n",
      "(array([0.00061006]), 0)\n",
      "(array([0.98877863]), 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid 함수\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "# 수치미분 함수\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad\n",
    "        \n",
    "class  LogicGate:\n",
    "    def __init__(self, gate_name, xdata, tdata):\n",
    "        self.name = gate_name\n",
    "        \n",
    "        # 입력 데이터, 정답 데이터 초기화     # batch 처리: 입력데이터와 정답데이터의 모든 경우의 수를 한 번에 처리하는 것\n",
    "        self.__xdata = xdata.reshape(4,2) # 4개의 입력데이터 x1, x2에 대하여 batch 처리 행렬\n",
    "        self.__tdata = tdata.reshape(4,1) # 4개의 입력데이터 x1, x2에 대한 각각의 계산값 행령\n",
    "        \n",
    "        # 2층 hidden layer unit : 6개 가정, 가중치 W2, 바이어스 b2 초기화\n",
    "        self.__W2 = np.random.rand(2,6) # weight, 2X6 matrix\n",
    "        self.__b2 = np.random.rand(6)\n",
    "        \n",
    "        # 3층 output layer unit : 1개, 가중치 W3, 바이어스 b3 초기화\n",
    "        self.__W3 = np.random.rand(6,1)\n",
    "        self.__b3 = np.random.rand(1)\n",
    "        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.__learning_rate = 1e-2\n",
    "        \n",
    "        print(self.name + \" object is created\")\n",
    "        \n",
    "    def feed_forward(self): # feed forward를 통하여 손실함수(cross-entropy) 값 계산\n",
    "        \n",
    "        delta = 1e-7 # log 무한대 발산 방지\n",
    "        \n",
    "        z2 = np.dot(self.__xdata, self.__W2) + self.__b2 # 은닉층의 선형회귀 값\n",
    "        a2 = sigmoid(z2) # 은닉층의 출력\n",
    "        \n",
    "        z3 = np.dot(a2, self.__W3) + self.__b3 # 출력층의 선형회귀 값\n",
    "        y = a3 = sigmoid(z3) # 출력층의 출력\n",
    "        \n",
    "        # cross-entropy\n",
    "        return -np.sum( self.__tdata * np.log(y + delta) + (1-self.__tdata) * np.log((1-y) + delta ))\n",
    "    \n",
    "    def loss_val(self): # 외부 출력을 윟나 손실함수(cross-entropy) 값 계산\n",
    "        \n",
    "        delta = 1e-7 # log 무한대 발산 방지\n",
    "        \n",
    "        z2 = np.dot(self.__xdata, self.__W2) + self.__b2 # 은닉층의 선형회귀 값\n",
    "        a2 = sigmoid(z2) # 은닉층의 출력\n",
    "        \n",
    "        z3 = np.dot(a2, self.__W3) + self.__b3 # 출력층의 선형회귀 값\n",
    "        y = a3 = sigmoid(z3) # 출력층의 출력\n",
    "        \n",
    "        # cross-entropy\n",
    "        return -np.sum( self.__tdata * np.log(y + delta) + (1-self.__tdata) * np.log((1-y) + delta ))\n",
    "    \n",
    "    # 수치미분을 이용하여 손실함수가 최소가 될 때까지 학습하는 함수\n",
    "    def train(self):\n",
    "        f = lambda x: self.feed_forward()\n",
    "        print(\"Initial loss value = \", self.loss_val())\n",
    "        \n",
    "        for step in range(10001):\n",
    "            self.__W2 -= self.__learning_rate + numerical_derivative(f, self.__W2)\n",
    "            self.__b2 -= self.__learning_rate + numerical_derivative(f, self.__b2)\n",
    "            self.__W3 -= self.__learning_rate + numerical_derivative(f, self.__W3)\n",
    "            self.__b3 -= self.__learning_rate + numerical_derivative(f, self.__b3)\n",
    "            \n",
    "            if(step % 400 == 0):\n",
    "                print(\"step = \", step, \" , loss value = \", self.loss_val())\n",
    "                \n",
    "    # query, 즉 미래 값 예측 함수\n",
    "    def predict(self, xdata):\n",
    "        z2 = np.dot(xdata, self.__W2) + self.__b2 # 은닉층의 선형회귀 값\n",
    "        a2 = sigmoid(z2) # 은닉층의 출력\n",
    "        \n",
    "        z3 = np.dot(a2, self.__W3) + self.__b3 # 출력층의 선형회귀 값\n",
    "        y = a3 = sigmoid(z3) # 출력층의 출력\n",
    "        \n",
    "        if y > 0.5:\n",
    "            result = 1 # True\n",
    "        else: \n",
    "            result = 0 # False\n",
    "            \n",
    "        return y, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01fad8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND object is created\n",
      "Initial loss value =  5.851566940227959\n",
      "step =  0  , loss value =  4.334256527358926\n",
      "step =  400  , loss value =  0.021197952870179282\n",
      "step =  800  , loss value =  0.01823005759233466\n",
      "step =  1200  , loss value =  0.01514204883968952\n",
      "step =  1600  , loss value =  0.014236555901946796\n",
      "step =  2000  , loss value =  0.01377134388795708\n",
      "step =  2400  , loss value =  0.013483275371649797\n",
      "step =  2800  , loss value =  0.01328670473970821\n",
      "step =  3200  , loss value =  0.01314381818798344\n",
      "step =  3600  , loss value =  0.013035176809163744\n",
      "step =  4000  , loss value =  0.012949736621819061\n",
      "step =  4400  , loss value =  0.012880751582136217\n",
      "step =  4800  , loss value =  0.012823865664195765\n",
      "step =  5200  , loss value =  0.012776139229567798\n",
      "step =  5600  , loss value =  0.012735515847097914\n",
      "step =  6000  , loss value =  0.012700513291322237\n",
      "step =  6400  , loss value =  0.012670035903823073\n",
      "step =  6800  , loss value =  0.012643256093075095\n",
      "step =  7200  , loss value =  0.01261953695019629\n",
      "step =  7600  , loss value =  0.012598380237620142\n",
      "step =  8000  , loss value =  0.012579390538718664\n",
      "step =  8400  , loss value =  0.012562249994596531\n",
      "step =  8800  , loss value =  0.01254670014927731\n",
      "step =  9200  , loss value =  0.012532528671375902\n",
      "step =  9600  , loss value =  0.012519559488982823\n",
      "step =  10000  , loss value =  0.012507645355109107\n"
     ]
    }
   ],
   "source": [
    "# AND Gate 객체 생성 및 training\n",
    "\n",
    "xdata = np.array([ [0,0], [0,1], [1,0], [1,1] ])\n",
    "tdata = np.array([0, 0, 0, 1])\n",
    "\n",
    "and_obj = LogicGate(\"AND\", xdata ,tdata)\n",
    "\n",
    "and_obj.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff3bd1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([3.51750385e-47]), 0)\n",
      "(array([0.00061074]), 0)\n",
      "(array([0.00061074]), 0)\n",
      "(array([0.98877725]), 1)\n"
     ]
    }
   ],
   "source": [
    "# AND Gate prediction\n",
    "test_data = np.array([ [0,0], [0,1], [1,0], [1,1] ])\n",
    "\n",
    "for data in test_data:\n",
    "    print(and_obj.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19cdabce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR object is created\n",
      "Initial loss value =  3.099011147747847\n",
      "step =  0  , loss value =  2.1712068680847523\n",
      "step =  400  , loss value =  0.02814315587656683\n",
      "step =  800  , loss value =  0.02247423290414957\n",
      "step =  1200  , loss value =  0.017345432847787792\n",
      "step =  1600  , loss value =  0.01630138873272673\n",
      "step =  2000  , loss value =  0.016871855564251633\n",
      "step =  2400  , loss value =  0.013866989631921945\n",
      "step =  2800  , loss value =  0.013387064627791498\n",
      "step =  3200  , loss value =  0.013286514812806524\n",
      "step =  3600  , loss value =  0.013275351952801358\n",
      "step =  4000  , loss value =  0.013328161560955495\n",
      "step =  4400  , loss value =  0.013463535447321491\n",
      "step =  4800  , loss value =  0.013756510394555178\n",
      "step =  5200  , loss value =  0.014561588703862222\n",
      "step =  5600  , loss value =  0.017581363362344185\n",
      "step =  6000  , loss value =  0.012189425918918917\n",
      "step =  6400  , loss value =  0.011971374979555898\n",
      "step =  6800  , loss value =  0.01191134190814984\n",
      "step =  7200  , loss value =  0.011889560241987113\n",
      "step =  7600  , loss value =  0.011871910689062746\n",
      "step =  8000  , loss value =  0.011856249919092555\n",
      "step =  8400  , loss value =  0.011842217308459201\n",
      "step =  8800  , loss value =  0.011829569711837122\n",
      "step =  9200  , loss value =  0.011818111221052193\n",
      "step =  9600  , loss value =  0.011807681291896004\n",
      "step =  10000  , loss value =  0.011798147076285579\n"
     ]
    }
   ],
   "source": [
    "# OR Gate 객체 생성 및 training\n",
    "\n",
    "xdata = np.array([ [0,0], [0,1], [1,0], [1,1] ])\n",
    "tdata = np.array([0, 1, 1, 1])\n",
    "\n",
    "or_obj = LogicGate(\"OR\", xdata, tdata)\n",
    "\n",
    "or_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3c9d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.00088865]), 0)\n",
      "(array([0.9963701]), 1)\n",
      "(array([0.9963701]), 1)\n",
      "(array([0.9963701]), 1)\n"
     ]
    }
   ],
   "source": [
    "# OR Gate prediction\n",
    "\n",
    "test_data = np.array([ [0,0], [0,1], [1,0], [1,1] ])\n",
    "\n",
    "for data in test_data:\n",
    "    print(or_obj.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09a12eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR object is created\n",
      "Initial loss value =  4.5535456495675\n",
      "step =  0  , loss value =  3.565762273608605\n",
      "step =  400  , loss value =  0.060651234983733565\n",
      "step =  800  , loss value =  0.03077584562125033\n",
      "step =  1200  , loss value =  0.025462767071104772\n",
      "step =  1600  , loss value =  0.022527070424841908\n",
      "step =  2000  , loss value =  0.020855793452105257\n",
      "step =  2400  , loss value =  0.019760425898191276\n",
      "step =  2800  , loss value =  0.018976027899830737\n",
      "step =  3200  , loss value =  0.018380944802749394\n",
      "step =  3600  , loss value =  0.01791099223668436\n",
      "step =  4000  , loss value =  0.017528756942783132\n",
      "step =  4400  , loss value =  0.017210762723561642\n",
      "step =  4800  , loss value =  0.016941430232705196\n",
      "step =  5200  , loss value =  0.016709959262147654\n",
      "step =  5600  , loss value =  0.016508596963620604\n",
      "step =  6000  , loss value =  0.01633161723055499\n",
      "step =  6400  , loss value =  0.016174689359819194\n",
      "step =  6800  , loss value =  0.016034471527484273\n",
      "step =  7200  , loss value =  0.015908340069018447\n",
      "step =  7600  , loss value =  0.01579420396167742\n",
      "step =  8000  , loss value =  0.01569037452547125\n",
      "step =  8400  , loss value =  0.015595471931901937\n",
      "step =  8800  , loss value =  0.015508356843273141\n",
      "step =  9200  , loss value =  0.01542807958915057\n",
      "step =  9600  , loss value =  0.015353841814888683\n",
      "step =  10000  , loss value =  0.015284967147731693\n"
     ]
    }
   ],
   "source": [
    "# XOR Gate 객체 생성 및 training\n",
    "# XOR: 두 입력값이 같지 않고, 1을 포함하면 1을 출력\n",
    "xdata = np.array([ [0,0], [0,1], [1,0], [1,1] ])\n",
    "tdata = np.array([0, 1, 1, 0])\n",
    "\n",
    "xor_obj = LogicGate(\"XOR\", xdata, tdata)\n",
    "\n",
    "xor_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6b697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b13c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97070496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
