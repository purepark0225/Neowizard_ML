{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593dca4d",
   "metadata": {},
   "source": [
    "회귀(Regression)\n",
    "- Training Data를 이용하여 데이터의 특성과 상관관계 등을 파악하고, 그 결과를 바탕으로 Training Data에 없는 미지의 데이터가 주어졌을 경우에, 그 결과를 연속적인 (숫자) 값으로 예측하는 것 ex) 공부시간과 시험성적 관계, 집 평수와 집 가격 관계 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1389dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.96175499]] , W.shape =  (1, 1) , b =  [0.10507992] , b.shape =  (1,)\n",
      "initial error value =  1.0223287910231302 initial W = [[0.96175499]] \n",
      " , b =  [0.10507992]\n",
      "step =  0 error value =  0.6521640325229414 W =  [[1.0238641]] , b =  [0.12154648]\n",
      "step =  400 error value =  0.008072445249440322 W =  [[1.05834441]] , b =  [0.78940971]\n",
      "step =  800 error value =  0.0005150697309913574 W =  [[1.0147377]] , b =  [0.94680525]\n",
      "step =  1200 error value =  3.2864493915507556e-05 W =  [[1.00372272]] , b =  [0.98656309]\n",
      "step =  1600 error value =  2.096949005804038e-06 W =  [[1.00094035]] , b =  [0.99660586]\n",
      "step =  2000 error value =  1.3379774367576475e-07 W =  [[1.00023753]] , b =  [0.99914265]\n",
      "step =  2400 error value =  8.537087055181934e-09 W =  [[1.00006]] , b =  [0.99978343]\n",
      "step =  2800 error value =  5.44716625170824e-10 W =  [[1.00001516]] , b =  [0.9999453]\n",
      "step =  3200 error value =  3.4756141036224934e-11 W =  [[1.00000383]] , b =  [0.99998618]\n",
      "step =  3600 error value =  2.2176472753679314e-12 W =  [[1.00000097]] , b =  [0.99999651]\n",
      "step =  4000 error value =  1.41499007063659e-13 W =  [[1.00000024]] , b =  [0.99999912]\n",
      "step =  4400 error value =  9.028473216281314e-15 W =  [[1.00000006]] , b =  [0.99999978]\n",
      "step =  4800 error value =  5.760699572259429e-16 W =  [[1.00000002]] , b =  [0.99999994]\n",
      "step =  5200 error value =  3.675667175219106e-17 W =  [[1.]] , b =  [0.99999999]\n",
      "step =  5600 error value =  2.3452924823805436e-18 W =  [[1.]] , b =  [1.]\n",
      "step =  6000 error value =  1.496435449361673e-19 W =  [[1.]] , b =  [1.]\n",
      "step =  6400 error value =  9.5481298369368e-21 W =  [[1.]] , b =  [1.]\n",
      "step =  6800 error value =  6.092291222920227e-22 W =  [[1.]] , b =  [1.]\n",
      "step =  7200 error value =  3.8870519243337716e-23 W =  [[1.]] , b =  [1.]\n",
      "step =  7600 error value =  2.4826887230861387e-24 W =  [[1.]] , b =  [1.]\n",
      "step =  8000 error value =  1.591170705584684e-25 W =  [[1.]] , b =  [1.]\n"
     ]
    }
   ],
   "source": [
    "# 1) 학습데이터(Training Data) 준비\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([1, 2, 3, 4, 5]).reshape(5,1)\n",
    "t_data = np.array([2, 3, 4, 5, 6]).reshape(5,1)\n",
    "\n",
    "# raw_data = [ [1, 2], [2, 3], [3, 4] [4, 5], [5, 6]]\n",
    "\n",
    "# 2) 임의의 직선 y = Wx + b 정의 (임의의 값으로 가중치 W, 바이어스 b 초기화)\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)\n",
    "\n",
    "# 3) 손실함수 E(W, b) 정의\n",
    "def loss_func(x, t):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return (np.sum( ( t - y)**2 ) ) / ( len(x) )\n",
    "\n",
    "\n",
    "# 4) 수치미분 numerical_derivative 및 utility 함수 정의\n",
    "def numerical_derivative(f,x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad\n",
    "\n",
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t : numpy type\n",
    "\n",
    "def error_val(x, t):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return (np.sum((t-y)**2)) / (len(x))\n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래값 예측 함수\n",
    "# 입력변수 x: numpy type\n",
    "def predict(x):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return y\n",
    "\n",
    "# 5) 학습율 (learning rate) 초기화 및 손실함수가 최소가 될 때까지 W, b 업데이트\n",
    "\n",
    "learning_rate = 1e-2 # 발산하는 경우, 1e-3 ~ 1e-6 등으로 바꾸어서 실행\n",
    "\n",
    "f = lambda x : loss_func(x_data, t_data) # f(x) = loss_func(x_data, t_data)\n",
    "\n",
    "print(\"initial error value = \", error_val(x_data, t_data), \"initial W =\", W, \"\\n\", \", b = \", b )\n",
    "\n",
    "for step in range(8001):\n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0): # 400번째마다 손실함수값, 가중치 W, bias b의  현재값을 보여주는 코드, 머싱러닝에서 디버깅용으로 많이 사용\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cc3ef68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57.48660748]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(43) # 43에 대한 미래값 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dd08b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.20488841]\n",
      " [0.47251166]\n",
      " [0.07669246]] , W.shape =  (3, 1) , b =  [0.79878771] , b.shape =  (1,)\n",
      "initial error value =  10479.830776468289 initial W = [[0.20488841]\n",
      " [0.47251166]\n",
      " [0.07669246]] \n",
      " , b =  [0.79878771]\n",
      "step =  0 error value =  3886.319921206669 W =  [[0.36825737]\n",
      " [0.63666612]\n",
      " [0.24511799]] , b =  [0.80001891]\n",
      "step =  400 error value =  14.82482125154867 W =  [[0.59454523]\n",
      " [0.82389701]\n",
      " [0.59871989]] , b =  [0.80180877]\n",
      "step =  800 error value =  12.357852979589818 W =  [[0.57018768]\n",
      " [0.768012  ]\n",
      " [0.67700595]] , b =  [0.80158753]\n",
      "step =  1200 error value =  10.611033531867132 W =  [[0.54841857]\n",
      " [0.72196423]\n",
      " [0.74316442]] , b =  [0.80128114]\n",
      "step =  1600 error value =  9.372110410690546 W =  [[0.52894744]\n",
      " [0.68408134]\n",
      " [0.79911164]] , b =  [0.80090274]\n",
      "step =  2000 error value =  8.49176366099731 W =  [[0.51151867]\n",
      " [0.65297006]\n",
      " [0.84645745]] , b =  [0.8004634]\n",
      "step =  2400 error value =  7.864877564435639 W =  [[0.49590699]\n",
      " [0.62747011]\n",
      " [0.88655447]] , b =  [0.79997246]\n",
      "step =  2800 error value =  7.417398355551893 W =  [[0.4819136 ]\n",
      " [0.6066157 ]\n",
      " [0.92053962]] , b =  [0.79943778]\n",
      "step =  3200 error value =  7.0971095413653735 W =  [[0.46936286]\n",
      " [0.58960326]\n",
      " [0.94936876]] , b =  [0.79886601]\n",
      "step =  3600 error value =  6.867155142222507 W =  [[0.45809934]\n",
      " [0.57576462]\n",
      " [0.97384581]] , b =  [0.79826276]\n",
      "step =  4000 error value =  6.701490746405764 W =  [[0.4479854 ]\n",
      " [0.56454446]\n",
      " [0.99464715]] , b =  [0.79763276]\n",
      "step =  4400 error value =  6.581687308628869 W =  [[0.43889894]\n",
      " [0.5554816 ]\n",
      " [1.01234208]] , b =  [0.79698001]\n",
      "step =  4800 error value =  6.494684395310718 W =  [[0.43073156]\n",
      " [0.54819327]\n",
      " [1.02740993]] , b =  [0.79630792]\n",
      "step =  5200 error value =  6.431210002392468 W =  [[0.42338692]\n",
      " [0.54236203]\n",
      " [1.04025451]] , b =  [0.79561934]\n",
      "step =  5600 error value =  6.384668502569632 W =  [[0.41677929]\n",
      " [0.53772489]\n",
      " [1.05121613]] , b =  [0.7949167]\n",
      "step =  6000 error value =  6.35035748147648 W =  [[0.41083229]\n",
      " [0.5340641 ]\n",
      " [1.06058173]] , b =  [0.79420206]\n",
      "step =  6400 error value =  6.324915740022652 W =  [[0.40547787]\n",
      " [0.53119961]\n",
      " [1.0685934 ]] , b =  [0.79347717]\n",
      "step =  6800 error value =  6.3059338597296595 W =  [[0.40065527]\n",
      " [0.52898273]\n",
      " [1.07545549]] , b =  [0.79274352]\n",
      "step =  7200 error value =  6.291679155234615 W =  [[0.39631023]\n",
      " [0.52729075]\n",
      " [1.0813406 ]] , b =  [0.79200237]\n",
      "step =  7600 error value =  6.280901170571879 W =  [[0.39239426]\n",
      " [0.52602261]\n",
      " [1.0863946 ]] , b =  [0.79125479]\n",
      "step =  8000 error value =  6.2726939341993315 W =  [[0.38886398]\n",
      " [0.52509521]\n",
      " [1.09074087]] , b =  [0.7905017]\n",
      "step =  8400 error value =  6.266398248431755 W =  [[0.38568055]\n",
      " [0.5244403 ]\n",
      " [1.09448382]] , b =  [0.78974388]\n",
      "step =  8800 error value =  6.261532246871707 W =  [[0.38280917]\n",
      " [0.52400198]\n",
      " [1.09771191]] , b =  [0.78898199]\n",
      "step =  9200 error value =  6.257741936061783 W =  [[0.38021866]\n",
      " [0.52373453]\n",
      " [1.10050009]] , b =  [0.78821661]\n",
      "step =  9600 error value =  6.254765884918485 W =  [[0.37788104]\n",
      " [0.5236007 ]\n",
      " [1.10291197]] , b =  [0.78744822]\n",
      "step =  10000 error value =  6.252409946141946 W =  [[0.3757712 ]\n",
      " [0.52357018]\n",
      " [1.10500155]] , b =  [0.78667724]\n"
     ]
    }
   ],
   "source": [
    "# multi-variable regression example\n",
    "\n",
    "# 1) 학습데이터(Training Data) 준비\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "loaded_data = np.loadtxt('./data-01-test-score.csv', delimiter = ',', dtype = np.float32)\n",
    "\n",
    "x_data = loaded_data[ :, 0:-1] # 모든 행에 대하여 1열부터 3열까지의 데이터를 입력데이터로\n",
    "t_data = loaded_data[ :, [-1]] # 모든 행에 대하여 마지막 1열(4열) 데이터\n",
    "\n",
    "# 2) 임의의 직선 y =  W1x1 + W2x2 + W3x3 + b 정의\n",
    "\n",
    "W = np.random.rand(3,1) # 3X1 행령\n",
    "b = np.random.rand(1)\n",
    "\n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)\n",
    "\n",
    "# 손실함수 E(W, b) 정의\n",
    "def loss_func(x, t):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return( np.sum( (t - y) **2 ) ) / (len(x))\n",
    "\n",
    "# 4) 수치미분 numerical_derivative 및 utility 함수 정의\n",
    "def numerical_derivative(f,x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad\n",
    "\n",
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t : numpy type\n",
    "\n",
    "def error_val(x, t):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return (np.sum((t-y)**2)) / (len(x))\n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래값 예측 함수\n",
    "# 입력변수 x: numpy type\n",
    "def predict(x):\n",
    "    y = np.dot(x, W) + b\n",
    "    \n",
    "    return y\n",
    "\n",
    "# 5) 학습율 (learning rate) 초기화 및 손실함수가 최소가 될 때까지 W, b 업데이트\n",
    "\n",
    "learning_rate = 1e-5 # 1e-2~1e-4 은 손실함수 값 발산(손실함수의 최솟값을 지나 엄청나게 커짐)\n",
    "\n",
    "f = lambda x : loss_func(x_data, t_data) # f(x) = loss_func(x_data, t_data)\n",
    "\n",
    "print(\"initial error value = \", error_val(x_data, t_data), \"initial W =\", W, \"\\n\", \", b = \", b )\n",
    "\n",
    "for step in range(10001):\n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0): # 400번째마다 손실함수값, 가중치 W, bias b의  현재값을 보여주는 코드, 머싱러닝에서 디버깅용으로 많이 사용\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62e1ce5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([179.17879965])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([100, 98, 81])\n",
    "\n",
    "predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
